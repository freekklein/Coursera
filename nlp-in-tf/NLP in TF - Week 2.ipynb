{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: https://www.coursera.org/learn/natural-language-processing-tensorflow/home/week/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings: words and associated words are clustered in \n",
    "# vectors in multi-dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbd, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = imbd['train'], imbd['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = []\n",
    "train_labels = []\n",
    "\n",
    "for i,j in train:\n",
    "    train_sentences.append(str(i.numpy()))\n",
    "    train_labels.append(str(j.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = []\n",
    "test_labels = []\n",
    "\n",
    "for i,j in test:\n",
    "    test_sentences.append(str(i.numpy()))\n",
    "    test_labels.append(str(j.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_labels_array = np.array(train_labels)\n",
    "test_labels_array = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert str label to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_str_labels_to_int = {'1':1, '0':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_int=list(map(int, train_labels))\n",
    "test_labels_int=list(map(int, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_array_int = np.array(train_labels_int)\n",
    "test_labels_array_int = np.array(test_labels_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tokenizer and apply padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = int(1e+4)  # size of vocabulary\n",
    "OOV_TOK = \"<OOV>\"\n",
    "MAX_LEN = 120  # max number of items in sequence\n",
    "TRUNC_TYPE = 'post'\n",
    "EMBED_DIM = 16  # embedding dimension (= output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token=OOV_TOK, num_words=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index # 86539 length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = pad_sequences(sequences, maxlen=MAX_LEN, truncating=TRUNC_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 120)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_padded = pad_sequences(test_sequences, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 120)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse word index tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_reversed = [(value,key) for (key,value) in word_index.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With flatten layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    # Result is 2D array with length of sentence and dimension of embedding\n",
    "    tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, \n",
    "                                  output_dim=EMBED_DIM,\n",
    "                                  input_length=MAX_LEN),\n",
    "    \n",
    "    # In NLP another layer type than flatten is used (due to size of output vector)\n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    # Dense NN for classification\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    model\n",
    "    .compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With global average pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, \n",
    "                                  output_dim=EMBED_DIM,\n",
    "                                  input_length=MAX_LEN),\n",
    "    \n",
    "    # Averages across the vectors to flatten it out (bit faster)\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6)                 102       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 160,109\n",
      "Trainable params: 160,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    model\n",
    "    .compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - ETA: 5:16 - loss: 0.6897 - accuracy: 0.59 - ETA: 29s - loss: 0.6937 - accuracy: 0.4922 - ETA: 18s - loss: 0.6940 - accuracy: 0.476 - ETA: 14s - loss: 0.6938 - accuracy: 0.487 - ETA: 10s - loss: 0.6929 - accuracy: 0.498 - ETA: 9s - loss: 0.6929 - accuracy: 0.506 - ETA: 8s - loss: 0.6930 - accuracy: 0.50 - ETA: 7s - loss: 0.6928 - accuracy: 0.50 - ETA: 7s - loss: 0.6926 - accuracy: 0.50 - ETA: 6s - loss: 0.6924 - accuracy: 0.51 - ETA: 6s - loss: 0.6920 - accuracy: 0.52 - ETA: 5s - loss: 0.6918 - accuracy: 0.52 - ETA: 5s - loss: 0.6915 - accuracy: 0.52 - ETA: 5s - loss: 0.6913 - accuracy: 0.52 - ETA: 5s - loss: 0.6906 - accuracy: 0.53 - ETA: 4s - loss: 0.6896 - accuracy: 0.53 - ETA: 4s - loss: 0.6886 - accuracy: 0.54 - ETA: 4s - loss: 0.6875 - accuracy: 0.55 - ETA: 4s - loss: 0.6855 - accuracy: 0.55 - ETA: 4s - loss: 0.6840 - accuracy: 0.56 - ETA: 4s - loss: 0.6814 - accuracy: 0.56 - ETA: 4s - loss: 0.6787 - accuracy: 0.57 - ETA: 3s - loss: 0.6755 - accuracy: 0.57 - ETA: 3s - loss: 0.6719 - accuracy: 0.58 - ETA: 3s - loss: 0.6668 - accuracy: 0.59 - ETA: 3s - loss: 0.6634 - accuracy: 0.59 - ETA: 3s - loss: 0.6599 - accuracy: 0.60 - ETA: 3s - loss: 0.6554 - accuracy: 0.60 - ETA: 3s - loss: 0.6531 - accuracy: 0.60 - ETA: 3s - loss: 0.6474 - accuracy: 0.61 - ETA: 3s - loss: 0.6423 - accuracy: 0.62 - ETA: 3s - loss: 0.6368 - accuracy: 0.62 - ETA: 3s - loss: 0.6318 - accuracy: 0.63 - ETA: 3s - loss: 0.6272 - accuracy: 0.63 - ETA: 3s - loss: 0.6232 - accuracy: 0.63 - ETA: 2s - loss: 0.6194 - accuracy: 0.64 - ETA: 2s - loss: 0.6142 - accuracy: 0.64 - ETA: 2s - loss: 0.6089 - accuracy: 0.65 - ETA: 2s - loss: 0.6046 - accuracy: 0.65 - ETA: 2s - loss: 0.5999 - accuracy: 0.65 - ETA: 2s - loss: 0.5938 - accuracy: 0.66 - ETA: 2s - loss: 0.5904 - accuracy: 0.66 - ETA: 2s - loss: 0.5871 - accuracy: 0.66 - ETA: 2s - loss: 0.5826 - accuracy: 0.67 - ETA: 2s - loss: 0.5775 - accuracy: 0.67 - ETA: 2s - loss: 0.5730 - accuracy: 0.68 - ETA: 2s - loss: 0.5709 - accuracy: 0.68 - ETA: 2s - loss: 0.5661 - accuracy: 0.68 - ETA: 2s - loss: 0.5633 - accuracy: 0.68 - ETA: 2s - loss: 0.5615 - accuracy: 0.69 - ETA: 1s - loss: 0.5587 - accuracy: 0.69 - ETA: 1s - loss: 0.5561 - accuracy: 0.69 - ETA: 1s - loss: 0.5536 - accuracy: 0.69 - ETA: 1s - loss: 0.5502 - accuracy: 0.69 - ETA: 1s - loss: 0.5470 - accuracy: 0.70 - ETA: 1s - loss: 0.5440 - accuracy: 0.70 - ETA: 1s - loss: 0.5417 - accuracy: 0.70 - ETA: 1s - loss: 0.5395 - accuracy: 0.70 - ETA: 1s - loss: 0.5369 - accuracy: 0.70 - ETA: 1s - loss: 0.5342 - accuracy: 0.71 - ETA: 1s - loss: 0.5321 - accuracy: 0.71 - ETA: 1s - loss: 0.5286 - accuracy: 0.71 - ETA: 1s - loss: 0.5264 - accuracy: 0.71 - ETA: 1s - loss: 0.5244 - accuracy: 0.71 - ETA: 1s - loss: 0.5219 - accuracy: 0.72 - ETA: 1s - loss: 0.5199 - accuracy: 0.72 - ETA: 1s - loss: 0.5182 - accuracy: 0.72 - ETA: 0s - loss: 0.5154 - accuracy: 0.72 - ETA: 0s - loss: 0.5130 - accuracy: 0.72 - ETA: 0s - loss: 0.5110 - accuracy: 0.72 - ETA: 0s - loss: 0.5091 - accuracy: 0.73 - ETA: 0s - loss: 0.5076 - accuracy: 0.73 - ETA: 0s - loss: 0.5052 - accuracy: 0.73 - ETA: 0s - loss: 0.5026 - accuracy: 0.73 - ETA: 0s - loss: 0.5003 - accuracy: 0.73 - ETA: 0s - loss: 0.4984 - accuracy: 0.73 - ETA: 0s - loss: 0.4967 - accuracy: 0.74 - ETA: 0s - loss: 0.4935 - accuracy: 0.74 - ETA: 0s - loss: 0.4920 - accuracy: 0.74 - ETA: 0s - loss: 0.4901 - accuracy: 0.74 - ETA: 0s - loss: 0.4886 - accuracy: 0.74 - ETA: 0s - loss: 0.4870 - accuracy: 0.74 - 6s 237us/sample - loss: 0.4853 - accuracy: 0.7490 - val_loss: 0.3619 - val_accuracy: 0.8383\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - ETA: 0s - loss: 0.4594 - accuracy: 0.81 - ETA: 7s - loss: 0.3081 - accuracy: 0.87 - ETA: 6s - loss: 0.2720 - accuracy: 0.90 - ETA: 5s - loss: 0.2621 - accuracy: 0.89 - ETA: 4s - loss: 0.2387 - accuracy: 0.90 - ETA: 4s - loss: 0.2270 - accuracy: 0.91 - ETA: 4s - loss: 0.2295 - accuracy: 0.91 - ETA: 4s - loss: 0.2262 - accuracy: 0.91 - ETA: 4s - loss: 0.2270 - accuracy: 0.91 - ETA: 4s - loss: 0.2314 - accuracy: 0.91 - ETA: 4s - loss: 0.2336 - accuracy: 0.91 - ETA: 4s - loss: 0.2328 - accuracy: 0.91 - ETA: 3s - loss: 0.2372 - accuracy: 0.90 - ETA: 3s - loss: 0.2367 - accuracy: 0.90 - ETA: 3s - loss: 0.2398 - accuracy: 0.90 - ETA: 3s - loss: 0.2416 - accuracy: 0.90 - ETA: 3s - loss: 0.2444 - accuracy: 0.90 - ETA: 3s - loss: 0.2445 - accuracy: 0.90 - ETA: 3s - loss: 0.2427 - accuracy: 0.90 - ETA: 3s - loss: 0.2444 - accuracy: 0.90 - ETA: 3s - loss: 0.2429 - accuracy: 0.91 - ETA: 3s - loss: 0.2417 - accuracy: 0.91 - ETA: 3s - loss: 0.2411 - accuracy: 0.91 - ETA: 3s - loss: 0.2421 - accuracy: 0.91 - ETA: 3s - loss: 0.2416 - accuracy: 0.91 - ETA: 3s - loss: 0.2411 - accuracy: 0.91 - ETA: 3s - loss: 0.2404 - accuracy: 0.91 - ETA: 2s - loss: 0.2403 - accuracy: 0.91 - ETA: 2s - loss: 0.2400 - accuracy: 0.91 - ETA: 2s - loss: 0.2397 - accuracy: 0.91 - ETA: 2s - loss: 0.2380 - accuracy: 0.91 - ETA: 2s - loss: 0.2388 - accuracy: 0.91 - ETA: 2s - loss: 0.2386 - accuracy: 0.91 - ETA: 2s - loss: 0.2391 - accuracy: 0.91 - ETA: 2s - loss: 0.2386 - accuracy: 0.91 - ETA: 2s - loss: 0.2374 - accuracy: 0.91 - ETA: 2s - loss: 0.2389 - accuracy: 0.91 - ETA: 2s - loss: 0.2389 - accuracy: 0.91 - ETA: 2s - loss: 0.2392 - accuracy: 0.91 - ETA: 2s - loss: 0.2398 - accuracy: 0.91 - ETA: 2s - loss: 0.2398 - accuracy: 0.91 - ETA: 2s - loss: 0.2391 - accuracy: 0.91 - ETA: 2s - loss: 0.2385 - accuracy: 0.91 - ETA: 2s - loss: 0.2381 - accuracy: 0.91 - ETA: 2s - loss: 0.2380 - accuracy: 0.91 - ETA: 1s - loss: 0.2379 - accuracy: 0.91 - ETA: 1s - loss: 0.2378 - accuracy: 0.91 - ETA: 1s - loss: 0.2374 - accuracy: 0.91 - ETA: 1s - loss: 0.2369 - accuracy: 0.91 - ETA: 1s - loss: 0.2363 - accuracy: 0.91 - ETA: 1s - loss: 0.2361 - accuracy: 0.91 - ETA: 1s - loss: 0.2357 - accuracy: 0.91 - ETA: 1s - loss: 0.2359 - accuracy: 0.91 - ETA: 1s - loss: 0.2362 - accuracy: 0.91 - ETA: 1s - loss: 0.2368 - accuracy: 0.91 - ETA: 1s - loss: 0.2364 - accuracy: 0.91 - ETA: 1s - loss: 0.2366 - accuracy: 0.91 - ETA: 1s - loss: 0.2365 - accuracy: 0.91 - ETA: 1s - loss: 0.2357 - accuracy: 0.91 - ETA: 1s - loss: 0.2355 - accuracy: 0.91 - ETA: 1s - loss: 0.2354 - accuracy: 0.91 - ETA: 1s - loss: 0.2356 - accuracy: 0.91 - ETA: 1s - loss: 0.2359 - accuracy: 0.91 - ETA: 1s - loss: 0.2358 - accuracy: 0.91 - ETA: 1s - loss: 0.2355 - accuracy: 0.91 - ETA: 1s - loss: 0.2359 - accuracy: 0.91 - ETA: 1s - loss: 0.2363 - accuracy: 0.91 - ETA: 1s - loss: 0.2356 - accuracy: 0.91 - ETA: 1s - loss: 0.2361 - accuracy: 0.91 - ETA: 1s - loss: 0.2360 - accuracy: 0.91 - ETA: 1s - loss: 0.2356 - accuracy: 0.91 - ETA: 1s - loss: 0.2360 - accuracy: 0.91 - ETA: 0s - loss: 0.2361 - accuracy: 0.91 - ETA: 0s - loss: 0.2368 - accuracy: 0.91 - ETA: 0s - loss: 0.2366 - accuracy: 0.91 - ETA: 0s - loss: 0.2367 - accuracy: 0.91 - ETA: 0s - loss: 0.2368 - accuracy: 0.91 - ETA: 0s - loss: 0.2360 - accuracy: 0.91 - ETA: 0s - loss: 0.2355 - accuracy: 0.91 - ETA: 0s - loss: 0.2357 - accuracy: 0.91 - ETA: 0s - loss: 0.2357 - accuracy: 0.91 - ETA: 0s - loss: 0.2360 - accuracy: 0.91 - ETA: 0s - loss: 0.2364 - accuracy: 0.91 - ETA: 0s - loss: 0.2374 - accuracy: 0.90 - ETA: 0s - loss: 0.2380 - accuracy: 0.90 - ETA: 0s - loss: 0.2377 - accuracy: 0.90 - ETA: 0s - loss: 0.2376 - accuracy: 0.90 - ETA: 0s - loss: 0.2373 - accuracy: 0.90 - ETA: 0s - loss: 0.2368 - accuracy: 0.90 - ETA: 0s - loss: 0.2361 - accuracy: 0.91 - ETA: 0s - loss: 0.2365 - accuracy: 0.90 - ETA: 0s - loss: 0.2367 - accuracy: 0.90 - ETA: 0s - loss: 0.2363 - accuracy: 0.90 - ETA: 0s - loss: 0.2361 - accuracy: 0.90 - 6s 250us/sample - loss: 0.2362 - accuracy: 0.9099 - val_loss: 0.3699 - val_accuracy: 0.8394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25485d93148>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    model\n",
    "    .fit(x=padded,\n",
    "         y=train_labels_array_int,\n",
    "         epochs=NUM_EPOCHS,\n",
    "         validation_data=(test_padded, test_labels_array_int)\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
